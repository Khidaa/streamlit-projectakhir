# -*- coding: utf-8 -*-
"""Salinan_dari_Proyek_Akhir_(1) (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1on366sS_ei8SvY_FaGOCrSebpUTZroLI

## **Data Collecting**
"""

# Import library yang diperlukan
import streamlit as st
import os  # Library untuk berinteraksi dengan sistem operasi
import pandas as pd  # Library untuk manipulasi dan analisis data
import numpy as np  # Library untuk komputasi numerik

from sklearn.preprocessing import StandardScaler, OneHotEncoder  # Import library preprocessing
from sklearn.model_selection import train_test_split  # Import library untuk pemisahan data
import matplotlib.pyplot as plt  # Library untuk membuat plot dan visualisasi data
from datetime import datetime, date  # Library untuk bekerja dengan data tanggal dan waktu
from statsmodels.tsa.seasonal import seasonal_decompose  # Library untuk dekomposisi musiman pada data deret waktu
from statsmodels.tsa.stattools import adfuller  # Library untuk uji Augmented Dickey-Fuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf  # Library untuk plot ACF dan PACF
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing  # Library untuk model peramalan eksponensial
from statsmodels.tsa.arima.model import ARIMA  # Library untuk model ARIMA
from pmdarima.arima import auto_arima  # Library untuk menentukan model ARIMA secara otomatis
from statsmodels.tsa.statespace.sarimax import SARIMAX  # Library untuk model SARIMA
from prophet import Prophet  # Library untuk model peramalan time series yang disebut Prophet
from prophet.plot import plot_plotly, plot_components_plotly  # Library untuk membuat plot interaktif menggunakan plotly
from tensorflow.keras.models import Sequential  # Library untuk membuat model sequential di TensorFlow
from tensorflow.keras.layers import *  # Import semua layer yang mungkin diperlukan dalam jaringan saraf
from tensorflow.keras.callbacks import ModelCheckpoint  # Library untuk menyimpan model selama pelatihan
from tensorflow.keras.losses import MeanSquaredError  # Library untuk menghitung nilai kerugian dengan Mean Squared Error
from tensorflow.keras.metrics import RootMeanSquaredError  # Library untuk mengukur kinerja model dengan Root Mean Squared Error
from tensorflow.keras.optimizers import Adam  # Optimizer Adam untuk mengoptimalkan model jaringan saraf
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error  # Library untuk menghitung metrik evaluasi seperti Mean Squared Error dan Mean Absolute Percentage Error
import warnings  # Library untuk mengelola peringatan
warnings.filterwarnings("ignore")  # Mengabaikan semua peringatan yang muncul selama runtime

from google.colab import drive
drive.mount('/content/drive')

# Baca dataset
data1 = pd.read_csv('/content/drive/MyDrive/Dataset/ispu_dki1.csv')
data2 = pd.read_csv('/content/drive/MyDrive/Dataset/ispu_dki2.csv')
data3 = pd.read_csv('/content/drive/MyDrive/Dataset/ispu_dki3.csv')
data4 = pd.read_csv('/content/drive/MyDrive/Dataset/ispu_dki4.csv')
data5 = pd.read_csv('/content/drive/MyDrive/Dataset/ispu_dki5.csv')
data6 = pd.read_csv('/content/drive/MyDrive/Dataset/ispu_20.csv')

# Gabungkan data secara vertikal (tambahkan baris)
data_AQ = pd.concat([data1, data2, data3, data4, data5, data6], ignore_index=True)

# Simpan data yang telah digabungkan ke dalam file CSV dengan nama "data_AQ.csv"
output_dir = '/content/drive/MyDrive/Dataset'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
data_AQ.to_csv(os.path.join(output_dir, 'data_AQ1.csv'), index=False)
print("Data telah disimpan di:", os.path.join(output_dir, 'data_AQ1.csv'))


# Cetak lokasi file yang telah disimpan
print("Data telah disimpan di:", os.path.join(output_dir, '/content/drive/MyDrive/Dataset/data_AQ1.csv'))

# Melihat tipe data setiap kolom
data_AQ.dtypes

# Melihat jumlah baris dan kolom
data_AQ.shape

print(data_AQ.isnull().sum())

# Memeriksa keberadaan garis '---' dalam setiap kolom
for column in data_AQ.columns:
    if data_AQ[column].dtype == 'object':  # Hanya memeriksa kolom dengan tipe data objek (string)
        contains_dash = data_AQ[column].str.contains('---').any()
        if contains_dash:
            print(f"Garis '---' ditemukan dalam kolom {column}")
        else:
            print(f"Tidak ada garis '---' dalam kolom {column}")

# Mengganti garis '---' dengan nilai NaN dalam kolom-kolom yang mengandung garis tersebut
for column in data_AQ.columns:
    if data_AQ[column].dtype == 'object' and data_AQ[column].str.contains('---').any():
        data_AQ[column] = data_AQ[column].replace('---', np.nan)

# Menyimpan dataset setelah penggantian nilai garis '---' dengan NaN dengan nama file yang sama
data_AQ.to_csv('data_AQ.csv', index=False)

print(data_AQ.isnull().sum())

# Baca data CSV (Ganti 'data_AQ.csv' dengan nama file Anda)
data_AQ = pd.read_csv('data_AQ.csv')

# Daftar kolom untuk menangani nilai hilang
kolom_dengan_nilai_hilang = ['pm10', 'so2', 'co', 'o3', 'no2']

# Iterasi melalui setiap kolom dan ganti nilai hilang
for kolom in kolom_dengan_nilai_hilang:
    data_AQ[kolom].replace(to_replace=[None, np.NAN, ''], value=np.NAN, inplace=True)

# Hapus baris dengan nilai NaN (Opsional)
data_AQ.dropna(subset=kolom_dengan_nilai_hilang, inplace=True)

# Verifikasi tipe data (Opsional)
print(data_AQ.dtypes)

# Menginstall dan mengimpor library dateparser


import dateparser

# Mengonversi kolom tanggal menjadi string
# Pastikan kolom tanggal berisi string yang dapat di-parse menjadi tanggal
data_AQ['tanggal'] = data_AQ['tanggal'].astype(str)
# Tangani nilai-nilai yang tidak valid atau tidak dapat di-parse menjadi tanggal
data_AQ['tanggal'] = data_AQ['tanggal'].apply(lambda x: dateparser.parse(x) if dateparser.parse(x) is not None else np.nan)


# Mengubah tipe data kolom critical menjadi kategorikal
data_AQ['critical'] = data_AQ['critical'].astype('category')

# Mengubah tipe data kolom kategori menjadi kategorikal
data_AQ['categori'] = data_AQ['categori'].astype('category')
# Menyimpan dataset setelah perubahan tipe data
data_AQ.to_csv('data_AQ', index=False)

# Mengonversi kolom 'tanggal' menjadi tipe data datetime
data_AQ['tanggal'] = pd.to_datetime(data_AQ['tanggal'], errors='coerce')

# Mencetak jumlah nilai NaT dalam kolom 'tanggal'
print(data_AQ['tanggal'].isnull().sum())

# Jika ada nilai NaT yang tidak terkonversi dengan baik, Anda dapat memeriksanya
if data_AQ['tanggal'].isnull().sum() > 0:
    print("Nilai NaT yang tidak terkonversi dengan baik:")
    print(data_AQ[data_AQ['tanggal'].isnull()]['tanggal'])

    # Menghapus baris dengan nilai NaT dalam kolom 'tanggal'
    data_AQ.dropna(subset=['tanggal'], inplace=True)

# Menampilkan nilai-nilai unik dalam kolom 'stasiun'
unique_values = data_AQ['stasiun'].unique()
print(unique_values)

# Membuat filter untuk baris yang mengandung 'DKI' dalam kolom 'stasiun'
dki_filter = data_AQ['stasiun'].str.contains('DKI')

# Mengaplikasikan filter untuk memperbarui dataframe
data_AQ = data_AQ[dki_filter]

# Memeriksa nilai-nilai unik setelah membersihkan
print(data_AQ['stasiun'].unique())

# Mengganti nilai 'DKI5 (Kebon Jeruk) Jakarta Barat' dengan 'DKI5 (Kebon Jeruk)'
data_AQ['stasiun'] = data_AQ['stasiun'].replace('DKI5 (Kebon Jeruk) Jakarta Barat', 'DKI5 (Kebon Jeruk)')

# Mengonversi tipe data kolom stasiun menjadi string
data_AQ['stasiun'] = data_AQ['stasiun'].astype(str)

print(data_AQ['stasiun'].value_counts())

print(data_AQ.isnull().sum())

# Verifikasi tipe data
print(data_AQ.dtypes)

# Simpan DataFrame yang sudah dibersihkan ke file CSV baru
data_AQ.to_csv('data_AQ1.csv', index=False)

# Baca dataset
data = pd.read_csv('/content/data_AQ1.csv')

# Cetak informasi dataset awal
print(f"Jumlah baris awal: {data.shape[0]}")
print(f"Kolom: {data.columns}")

# Cek duplikasi data
duplikat = data.duplicated()

# Hitung jumlah baris duplikasi
jumlah_duplikat = duplikat.sum()

# Cetak informasi duplikasi
print(f"Jumlah baris duplikasi: {jumlah_duplikat}")

# Tampilkan beberapa contoh baris duplikat (opsional)
if jumlah_duplikat > 0:
    print("\nContoh baris duplikat:")
    data[duplikat].head()

# Hapus baris duplikasi
try:
    data_tanpa_duplikat = data.drop_duplicates()
except Exception as e:
    print(f"Gagal menghapus duplikasi: {e}")
    data_tanpa_duplikat = data

# Cetak informasi dataset setelah dihapus duplikasinya
print("\nInformasi dataset setelah dihapus duplikasi:")
print(f"Jumlah baris: {data_tanpa_duplikat.shape[0]}")
print(f"Kolom: {data_tanpa_duplikat.columns}")

# Simpan data tanpa duplikasi ke dataset baru
try:
    data_tanpa_duplikat.to_csv('data_AQ1.csv', index=False)
except Exception as e:
    print(f"Gagal menyimpan data: {e}")

"""Memanggil Dataset"""

df = pd.read_csv('/content/data_AQ1.csv')

"""Deskripsi Dataset"""

# Menentukan target kolom yang ingin dianalisis
target = "categori"

# Menghitung jumlah nilai unik dalam kolom target
val_counts = df[target].value_counts()

# Menampilkan nilai unik beserta jumlahnya
print(val_counts, "\n\n")

# Membuat plot untuk visualisasi jumlah data dalam kolom target
plt.figure(figsize=(15, 5))  # Mengatur ukuran gambar plot
plt.subplot(1, 2, 1)  # Membuat subplot pertama dalam grid 1x2
sns.countplot(x=data[target])  # Membuat plot countplot menggunakan library seaborn

plt.subplot(1, 2, 2)  # Membuat subplot kedua dalam grid 1x2
plt.pie(val_counts, labels=val_counts.keys(), autopct="%.2f%%")  # Membuat plot pie chart dengan label dan persentase

plt.tight_layout()  # Menyesuaikan layout plot agar terlihat rapi
plt.show()  # Menampilkan plot

# Menampilkan beberapa baris pertama dari DataFrame df
df.head()

# Menampilkan informasi rinci tentang DataFrame df
df.info()

"""Transform Data"""

# Mengubah kolom 'tanggal' menjadi tipe data datetime dengan format tahun-bulan-tanggal (%Y-%m-%d)
df['tanggal'] = pd.to_datetime(df['tanggal'], format='%Y-%m-%d')

# Mengatur kolom 'tanggal' sebagai indeks (index) DataFrame df
df.set_index(['tanggal'], inplace=True)

# Menampilkan beberapa baris pertama dari DataFrame df setelah melakukan pengaturan kolom 'tanggal' sebagai indeks
df.head()

# Mengetahui nilai minimum dan maksimum dari indeks (index) DataFrame df setelah pengaturan kolom 'tanggal' sebagai indeks
df.index.min(), df.index.max()

# Memeriksa apakah selisih antara nilai maksimum dan minimum dari indeks DataFrame df adalah 214
if (df.index.max() - df.index.min() == 214):
    print('Data is Continous')  # Jika selisihnya 214, maka mencetak pesan "Data is Continous"

"""## **Modelling**

### **Analisa Time Series**
"""

# Membuat plot dari DataFrame df
df.plot()

# Memecah DataFrame df menjadi dua bagian berdasarkan proporsi data train dan test (80% train, 20% test)
split = int(len(df) * 0.8)  # Menghitung titik pemisahan (80% data train, 20% data test)
train_df = df[:split]  # Memisahkan 80% data pertama sebagai data training
test_df = df[split:]  # Memisahkan 20% data terakhir sebagai data testing

# Mendefinisikan daftar p yang berisi nama-nama kolom yang ingin didekomposisi
p = ['pm10', 'so2', 'co', 'o3', 'no2']

# Melakukan dekomposisi musiman untuk setiap kolom dalam daftar p
for i in p:
    decompose_add = seasonal_decompose(df[i], period=30)  # Melakukan dekomposisi musiman dengan periode 30 hari
    decompose_add.plot()  # Menampilkan plot dari hasil dekomposisi musiman

# Menguji stasioneritas dari beberapa kolom dalam suatu dataframe time series
def adf_test(ts):
    columns_to_test = ['pm10', 'so2', 'co', 'o3', 'no2']  # Kolom-kolom yang akan diuji stasioneritasnya
    for column in columns_to_test:
        result = adfuller(ts[column])  # Melakukan uji Augmented Dickey-Fuller (ADF) pada setiap kolom
        print(f'ADF Statistic for {column}: {result[0]}')  # Menampilkan nilai ADF Statistic
        print(f'p-value for {column}: {result[1]}')  # Menampilkan nilai p-value
        print('Critical Values:')
        for key, value in result[4].items():  # Menampilkan nilai Critical Values
            print(f'   {key}: {value}')
        print('\n')

# Memanggil fungsi adf_test untuk DataFrame df
adf_test(df)

""" stasioner memiliki arti bahwa sifat statistik data tidak berubah seiring waktu"""

# Menghitung selisih (differencing) antara nilai pada beberapa kolom dalam DataFrame df
diff_df = df[p].diff()

# Menampilkan beberapa baris pertama dari DataFrame hasil differencing
diff_df.head()

# Menghapus baris dengan nilai NaN (kosong) dari DataFrame diff_df
diff_df.dropna(inplace=True)

# Menampilkan plot dari DataFrame diff_df dengan grid
diff_df.plot(grid=True)

# Menggunakan fungsi adf_test untuk menguji stasioneritas dari DataFrame diff_df setelah proses differencing
adf_test(diff_df)

# Looping melalui setiap kolom dalam daftar p
for i in p:
    # Menampilkan plot Autocorrelation Function (ACF) untuk kolom i dengan lag maksimal 30
    plot_acf(df[i], lags=30, title=f'Autocorrelation Function - {i}')
    plt.show()  # Menampilkan plot ACF

    # Menampilkan plot Partial Autocorrelation Function (PACF) untuk kolom i dengan lag maksimal 30
    plot_pacf(df[i], lags=30, title=f'Partial Autocorrelation Function - {i}')
    plt.show()  # Menampilkan plot PACF

df_train = train_df.loc[train_df['stasiun'] == 'DKI1 (Bunderan HI)']
df_test = test_df.loc[test_df['stasiun'] == 'DKI1 (Bunderan HI)']

df_train

df_test

"""### **Single Exponential Smoothing**"""

# Mendefinisikan dictionary untuk menyimpan model Simple Exponential Smoothing (SES) dan hasil prediksi
ses_model = {}
single_exp_train_pred = {}
single_exp_test_pred = {}

# Looping melalui setiap kolom dalam daftar p
for i in p:
    # Melatih model SES menggunakan data training untuk kolom i
    ses_model[i] = SimpleExpSmoothing(df_train[i]).fit()

    # Mendapatkan nilai prediksi untuk data training
    single_exp_train_pred[i] = ses_model[i].fittedvalues

    # Mendapatkan nilai prediksi untuk data testing (35 hari)
    single_exp_test_pred[i] = ses_model[i].forecast(35)

    # Menampilkan panjang prediksi untuk data training dan data testing
    print(f'len(single_exp_train_pred[{i}])={len(single_exp_train_pred[i])}')
    print(f'len(single_exp_test_pred[{i}])={len(single_exp_test_pred[i])}')

    # Plot data training, data testing, dan hasil prediksi SES
    df_train[i].plot(style='--', color='blue', legend=True, label=f'df_train - {i}')
    df_test[i].plot(style='--', color='green', legend=True, label=f'df_test - {i}')
    single_exp_test_pred[i].plot(color='yellow', legend=True, label=f'Prediction - {i}')
    single_exp_train_pred[i].plot(color='grey', legend=True, label=f'Prediction - {i}')
    plt.xlim('2010-01-01', '2020-12-11')  # Menentukan rentang sumbu x pada plot
    plt.show()  # Menampilkan plot

    # Menghitung Root Mean Squared Error (RMSE) untuk data training dan data testing
    print(f'Train RMSE - {i}:', mean_squared_error(df_train[i], single_exp_train_pred[i])**0.5)
    print(f'Test RMSE - {i}:', mean_squared_error(df_test[i], single_exp_test_pred[i])**0.5)

    # Menghitung Mean Absolute Percentage Error (MAPE) untuk data training dan data testing
    print(f'Train MAPE - {i}:', mean_absolute_percentage_error(df_train[i], single_exp_train_pred[i]))
    print(f'Test MAPE - {i}:', mean_absolute_percentage_error(df_test[i], single_exp_test_pred[i]))

"""### **Double Exponential Smoothing**"""

des_model = {}
double_exp_train_pred = {}
double_exp_test_pred = {}
for i in p:
  des_model[i] = double_exp = ExponentialSmoothing(df_train[i], trend=None, initialization_method='heuristic', seasonal='add', seasonal_periods=29, damped_trend=False).fit()
  double_exp_train_pred[i] = des_model[i].fittedvalues
  double_exp_test_pred[i] = des_model[i].forecast(35)
  print(f'len(double_exp_train_pred[{i}])={len(double_exp_train_pred[i])}')
  print(f'len(double_exp_test_pred[{i}])={len(double_exp_test_pred[i])}')
  df_train[i].plot(style='--', color='blue', legend=True, label=f'df_train - {i}')
  df_test[i].plot(style='--', color='green', legend=True, label=f'df_test - {i}')
  double_exp_test_pred[i].plot(color='yellow', legend=True, label=f'Prediction - {i}')
  double_exp_train_pred[i].plot(color='grey', legend=True, label=f'Prediction - {i}')
  plt.xlim('2010-01-01', '2020-12-11')
  plt.show()
  print(f'Train RMSE - {i}:',mean_squared_error(df_train[i], double_exp_train_pred[i])**0.5)
  print(f'Test RMSE - {i}:', mean_squared_error(df_test[i], double_exp_test_pred[i])**0.5)
  print(f'Train MAPE - {i}:', mean_absolute_percentage_error(df_train[i], double_exp_train_pred[i]))
  print(f'Test MAPE - {i}:', mean_absolute_percentage_error(df_test[i], double_exp_test_pred[i]))

"""### **SARIMA**"""

sarima_model = {}
sarima_train_pred = {}
sarima_test_pred = {}
for i in p:
    # Fitting SARIMA model
    sarima_model[i] = SARIMAX(df_train[i], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12)).fit()

    # Melakukan prediksi pada data pelatihan
    sarima_train_pred[i] = sarima_model[i].predict(start=df_train.index.min(), end=df_train.index.max(), dynamic=False)

    # Melakukan prediksi untuk data uji
    sarima_test_pred[i] = sarima_model[i].forecast(steps=len(df_test))

    # Memplot hasil prediksi
    plt.plot(df_train.index, df_train[i], label='Train Data')
    plt.plot(df_test.index, df_test[i], label='Test Data')
    plt.plot(df_test.index, sarima_test_pred[i], label='SARIMA Prediction')
    plt.title(f'SARIMA Prediction for {i}')
    plt.legend()
    plt.show()

    # Menghitung RMSE dan MAPE
    train_rmse = mean_squared_error(df_train[i], sarima_train_pred[i], squared=False)
    test_rmse = mean_squared_error(df_test[i], sarima_test_pred[i], squared=False)
    train_mape = mean_absolute_percentage_error(df_train[i], sarima_train_pred[i])
    test_mape = mean_absolute_percentage_error(df_test[i], sarima_test_pred[i])

    # Menampilkan hasil evaluasi
    print(f'Train RMSE for {i}: {train_rmse}')
    print(f'Test RMSE for {i}: {test_rmse}')
    print(f'Train MAPE for {i}: {train_mape}')
    print(f'Test MAPE for {i}: {test_mape}')

"""### **ARIMA**"""

arima_model = {}
arima_train_pred = {}
arima_test_pred = {}
for i in p:
    # Fitting ARIMA model
    arima_model[i] = ARIMA(df_train[i], order=(1, 1, 1)).fit()

    # Melakukan prediksi pada data pelatihan
    arima_train_pred[i] = arima_model[i].predict(start=df_train.index.min(), end=df_train.index.max(), dynamic=False)

    # Melakukan prediksi untuk data uji
    arima_test_pred[i] = arima_model[i].forecast(steps=len(df_test))

    # Memplot hasil prediksi
    plt.plot(df_train.index, df_train[i], label='Train Data')
    plt.plot(df_test.index, df_test[i], label='Test Data')
    plt.plot(df_test.index, arima_test_pred[i], label='ARIMA Prediction')
    plt.title(f'ARIMA Prediction for {i}')
    plt.legend()
    plt.show()

    # Menghitung RMSE dan MAPE
    train_rmse = mean_squared_error(df_train[i], arima_train_pred[i], squared=False)
    test_rmse = mean_squared_error(df_test[i], arima_test_pred[i], squared=False)
    train_mape = mean_absolute_percentage_error(df_train[i], arima_train_pred[i])
    test_mape = mean_absolute_percentage_error(df_test[i], arima_test_pred[i])

    # Menampilkan hasil evaluasi
    print(f'Train RMSE for {i}: {train_rmse}')
    print(f'Test RMSE for {i}: {test_rmse}')
    print(f'Train MAPE for {i}: {train_mape}')
    print(f'Test MAPE for {i}: {test_mape}')

# Facebook Prophet
prophet_model = {}
prophet_train_pred = {}
prophet_test_pred = {}
for i in p:
  df_prophet = pd.DataFrame()
  df_prophet['ds'] = df_train.index
  df_prophet['y'] = df_train[i].values
  prophet_model[i] = Prophet()
  prophet_model[i].fit(df_prophet)
  future = prophet_model[i].make_future_dataframe(periods=35, freq='D')
  prophet_train_pred[i] = prophet_model[i].predict(df_prophet)
  prophet_test_pred[i] = prophet_model[i].predict(future.tail(35))
  print(f'len(prophet_train_pred[{i}])={len(prophet_train_pred[i])}')
  print(f'len(prophet_test_pred[{i}])={len(prophet_test_pred[i])}')
  prophet_model[i].plot(prophet_train_pred[i], xlabel='Date', ylabel=i)
  prophet_model[i].plot(prophet_test_pred[i], xlabel='Date', ylabel=i)
  plt.show()
  print(f'Train RMSE - {i}:',mean_squared_error(df_train[i], prophet_train_pred[i]['yhat'][:len(df_train)])**0.5)
  print(f'Test RMSE - {i}:', mean_squared_error(df_test[i], prophet_test_pred[i]['yhat'])**0.5)
  print(f'Train MAPE - {i}:', mean_absolute_percentage_error(df_train[i], prophet_train_pred[i]['yhat'][:len(df_train)]))
  print(f'Test MAPE - {i}:', mean_absolute_percentage_error(df_test[i], prophet_test_pred[i]['yhat']))

# Naive Bayes
from sklearn.naive_bayes import GaussianNB

# Decision Tree
from sklearn.tree import DecisionTreeClassifier

# Support Vector Machine
from sklearn.svm import SVC

# Logistic Regression
from sklearn.linear_model import LogisticRegression

# K-Nearest Neighbors
from sklearn.neighbors import KNeighborsClassifier

# XGBoost
from xgboost import XGBClassifier
# AdaBoost
from sklearn.ensemble import AdaBoostClassifier

# Random Forest
from sklearn.ensemble import RandomForestClassifier

# Mengimpor library yang diperlukan untuk model LSTM
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from tensorflow.keras.optimizers import Adam

"""### **LTSM**"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense
import tensorflow.keras.callbacks as KC

# Data untuk LSTM
data_lstm = df_train['pm10'].values.reshape(-1, 1)

# Prapemrosesan data
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(data_lstm)

# Fungsi untuk membuat dataset dengan time step
def create_dataset(data, time_steps):
    X, y = [], []
    for i in range(len(data) - time_steps):
        X.append(data[i:(i + time_steps), 0])
        y.append(data[i + time_steps, 0])
    return np.array(X), np.array(y)

# Pembagian data latih dan uji
X_train, X_test, y_train, y_test = train_test_split(data_scaled, data_scaled, test_size=0.2)

# Time step dan pembuatan dataset
time_steps = 10
X_train, y_train = create_dataset(X_train, time_steps)
X_test, y_test = create_dataset(X_test, time_steps)

# Konfigurasi model
model = Sequential([
    LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)),
    Dropout(0.2),
    LSTM(units=50),
    Dropout(0.2),
    Dense(1)
])

# Early stopping callback
early_stopping = KC.EarlyStopping(monitor='val_loss', patience=5)

# Kompilasi model
model.compile(optimizer='adam', loss='mean_squared_error')

# Pelatihan model
history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), callbacks=[early_stopping])

# Plot loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()
plt.show()

# Menggunakan model LSTM untuk membuat prediksi
train_pred = model.predict(X_train)
test_pred = model.predict(X_test)

# Mengembalikan data yang telah dinormalisasi ke dalam skala aslinya
train_pred = scaler.inverse_transform(train_pred)
y_train = scaler.inverse_transform(y_train.reshape(-1, 1)).reshape(-1)

test_pred = scaler.inverse_transform(test_pred)
y_test = scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(-1)

# Menampilkan performa model LSTM
train_rmse = mean_squared_error(y_train, train_pred, squared=False)
test_rmse = mean_squared_error(y_test, test_pred, squared=False)

print("Train RMSE:", train_rmse)
print("Test RMSE:", test_rmse)

# Mengonversi test_pred agar sesuai dengan panjang timestamps
test_pred = np.resize(test_pred, len(timestamps))

# Kemudian plot data
plt.figure(figsize=(14, 7))
plt.plot(timestamps, y_test, label='Actual', color='blue')
plt.plot(timestamps, test_pred, label='Predicted', color='red')
plt.title('LSTM Model Test Prediction')
plt.xlabel('Timestamp')
plt.ylabel('Value')
plt.legend()
plt.show()

"""### **NAIVE BAYES**"""

df = pd.read_csv('/content/data_AQ1.csv')

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder

# Mengubah kategori menjadi one-hot encoding
X = pd.get_dummies(df.drop(columns=['categori', 'tanggal']))

# Mengubah label kategori menjadi bentuk numerik
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df['categori'])

# Pembagian data menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Pastikan 'tanggal' diubah menjadi tipe data datetime
#df['tanggal'] = pd.to_datetime(df['tanggal'])

# Mengekstrak tahun, bulan, dan hari dari kolom tanggal
#df['tahun'] = df['tanggal'].dt.year
#df['bulan'] = df['tanggal'].dt.month
#df['hari'] = df['tanggal'].dt.day

# Membuat fitur dan label
#X = df.drop(columns=['categori', 'tanggal'])  # Menghapus kolom 'categori' dan 'tanggal' dari fitur
#y = df['categori']

# Melatih model Naive Bayes
naive_bayes = GaussianNB()
naive_bayes.fit(X, y)

df.head()

# Membuat prediksi menggunakan model
naive_bayes_train_pred = naive_bayes.predict(X_train)
naive_bayes_test_pred = naive_bayes.predict(X_test)

# Mengukur kinerja model
train_rmse_naive_bayes = mean_squared_error(y_train, naive_bayes_train_pred, squared=False)
test_rmse_naive_bayes = mean_squared_error(y_test, naive_bayes_test_pred, squared=False)

# Menampilkan hasil
print("Train RMSE Naive Bayes:", train_rmse_naive_bayes)
print("Test RMSE Naive Bayes:", test_rmse_naive_bayes)

"""### **Decision Tree**"""

df = pd.read_csv('/content/data_AQ1.csv')

# Menentukan fitur (X) dan label target (y)
X = df.drop(columns=['categori'])
y = df['categori']

# Membagi data menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Melatih model Decision Tree
decision_tree = DecisionTreeClassifier()
decision_tree.fit(X_train, y_train)

decision_tree_train_pred = decision_tree.predict(X_train)
decision_tree_test_pred = decision_tree.predict(X_test)

train_rmse_decision_tree = mean_squared_error(y_train, decision_tree_train_pred, squared=False)
test_rmse_decision_tree = mean_squared_error(y_test, decision_tree_test_pred, squared=False)

print("Train RMSE Decision Tree:", train_rmse_decision_tree)
print("Test RMSE Decision Tree:", test_rmse_decision_tree)

"""### **SVM**"""

# Melatih model Support Vector Machine
svm = SVC()
svm.fit(X_train, y_train)

svm_train_pred = svm.predict(X_train)
svm_test_pred = svm.predict(X_test)

train_rmse_svm = mean_squared_error(y_train, svm_train_pred, squared=False)
test_rmse_svm = mean_squared_error(y_test, svm_test_pred, squared=False)

print("Train RMSE SVM:", train_rmse_svm)
print("Test RMSE SVM:", test_rmse_svm)

"""### **Logistic Regression**"""

# Melatih model Logistic Regression
logistic_regression = LogisticRegression()
logistic_regression.fit(X_train, y_train)

logistic_regression_train_pred = logistic_regression.predict(X_train)
logistic_regression_test_pred = logistic_regression.predict(X_test)

train_rmse_logistic_regression = mean_squared_error(y_train, logistic_regression_train_pred, squared=False)
test_rmse_logistic_regression = mean_squared_error(y_test, logistic_regression_test_pred, squared=False)

print("Train RMSE Logistic Regression:", train_rmse_logistic_regression)
print("Test RMSE Logistic Regression:", test_rmse_logistic_regression)

"""### **K-Nearest Neighbors**"""

# Melatih model K-Nearest Neighbors
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)

knn_train_pred = knn.predict(X_train)
knn_test_pred = knn.predict(X_test)

train_rmse_knn = mean_squared_error(y_train, knn_train_pred, squared=False)
test_rmse_knn = mean_squared_error(y_test, knn_test_pred, squared=False)

print("Train RMSE KNN:", train_rmse_knn)
print("Test RMSE KNN:", test_rmse_knn)

"""### **XGBoost**"""

# Melatih model XGBoost
xgboost = XGBClassifier()
xgboost.fit(X_train, y_train)

xgboost_train_pred = xgboost.predict(X_train)
xgboost_test_pred = xgboost.predict(X_test)

train_rmse_xgboost = mean_squared_error(y_train, xgboost_train_pred, squared=False)
test_rmse_xgboost = mean_squared_error(y_test, xgboost_test_pred, squared=False)

print("Train RMSE XGBoost:", train_rmse_xgboost)
print("Test RMSE XGBoost:", test_rmse_xgboost)

"""### **AdaBoost**"""

# Melatih model AdaBoost
adaboost = AdaBoostClassifier()
adaboost.fit(X_train, y_train)

adaboost_train_pred = adaboost.predict(X_train)
adaboost_test_pred = adaboost.predict(X_test)

train_rmse_adaboost = mean_squared_error(y_train, adaboost_train_pred, squared=False)
test_rmse_adaboost = mean_squared_error(y_test, adaboost_test_pred, squared=False)

print("Train RMSE AdaBoost:", train_rmse_adaboost)
print("Test RMSE AdaBoost:", test_rmse_adaboost)

"""### **Random Forest**"""

# Melatih model Random Forest
random_forest = RandomForestClassifier()
random_forest.fit(X_train, y_train)

random_forest_train_pred = random_forest.predict(X_train)
random_forest_test_pred = random_forest.predict(X_test)

train_rmse_random_forest = mean_squared_error(y_train, random_forest_train_pred, squared=False)
test_rmse_random_forest = mean_squared_error(y_test, random_forest_test_pred, squared=False)

print("Train RMSE Random Forest:", train_rmse_random_forest)
print("Test RMSE Random Forest:", test_rmse_random_forest)

# Mencari model dengan RMSE terendah pada data latih dan data uji
train_rmse_list = [train_rmse_naive_bayes, train_rmse_decision_tree, train_rmse_svm,
                   train_rmse_logistic_regression, train_rmse_knn, train_rmse_xgboost,
                   train_rmse_adaboost, train_rmse_random_forest, train_rmse]

test_rmse_list = [test_rmse_naive_bayes, test_rmse_decision_tree, test_rmse_svm,
                  test_rmse_logistic_regression, test_rmse_knn, test_rmse_xgboost,
                  test_rmse_adaboost, test_rmse_random_forest, test_rmse]

train_rmse_list_names = ["Naive Bayes", "Decision Tree", "SVM", "Logistic Regression",
                         "KNN", "XGBoost", "AdaBoost", "Random Forest", "LSTM"]

test_rmse_list_names = ["Naive Bayes", "Decision Tree", "SVM", "Logistic Regression",
                        "KNN", "XGBoost", "AdaBoost", "Random Forest", "LSTM"]

# Model dengan RMSE terendah pada data latih dan data uji
best_train_rmse = min(train_rmse_list)
best_train_rmse_model = train_rmse_list_names[train_rmse_list.index(best_train_rmse)]

best_test_rmse = min(test_rmse_list)
best_test_rmse_model = test_rmse_list_names[test_rmse_list.index(best_test_rmse)]

print("Best Train RMSE Model:", best_train_rmse_model)
print("Best Train RMSE:", best_train_rmse)

print("Best Test RMSE Model:", best_test_rmse_model)
print("Best Test RMSE:", best_test_rmse)